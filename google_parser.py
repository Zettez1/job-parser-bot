"""
–ü–æ–∏—Å–∫ —Ä–∞–±–æ—Ç–Ω–∏–∫–æ–≤ —á–µ—Ä–µ–∑ Google
–ò—â–µ—Ç –≤ Telegram –∏ –Ω–∞ —Å–∞–π—Ç–∞—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏—è –æ –ø–æ–∏—Å–∫–µ —Ä–∞–±–æ—Ç—ã
"""
import asyncio
import logging
import aiohttp
from bs4 import BeautifulSoup
from datetime import datetime
from telegram import Bot
import re
import urllib.parse

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
BOT_TOKEN = "8302303298:AAGH3Nllv4JaQRoi8Em8rO1-L_zGinN-gVM"
CHAT_ID = "-1003407248691"

# –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
SEARCH_QUERIES = [
    'site:t.me "—à—É–∫–∞—é —Ä–æ–±–æ—Ç—É" –∂–∏—Ç–æ–º–∏—Ä',
    'site:t.me "—à—É–∫–∞—é –ø—ñ–¥—Ä–æ–±—ñ—Ç–æ–∫" –∂–∏—Ç–æ–º–∏—Ä',
    'site:t.me —Å–≤–∞—Ä—â–∏–∫ –∂–∏—Ç–æ–º–∏—Ä "—à—É–∫–∞—é"',
    'site:t.me —Ä—ñ–∑–Ω–æ—Ä–æ–±–æ—á–∏–π –∂–∏—Ç–æ–º–∏—Ä',
    'site:olx.ua "—à—É–∫–∞—é —Ä–æ–±–æ—Ç—É" –∂–∏—Ç–æ–º–∏—Ä',
    'site:olx.ua —Å–≤–∞—Ä—â–∏–∫ –∂–∏—Ç–æ–º–∏—Ä —Ä–µ–∑—é–º–µ',
]

logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)


async def search_google(query, session):
    """–ü–æ–∏—Å–∫ –≤ Google"""
    results = []
    
    # –ö–æ–¥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
    encoded_query = urllib.parse.quote(query)
    url = f"https://www.google.com/search?q={encoded_query}&num=20&hl=uk"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'uk-UA,uk;q=0.9,en-US;q=0.8,en;q=0.7',
    }
    
    try:
        async with session.get(url, headers=headers, timeout=15) as response:
            if response.status == 200:
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # –ò—â–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
                for g in soup.find_all('div', class_='g'):
                    try:
                        # –°—Å—ã–ª–∫–∞
                        link_elem = g.find('a')
                        if not link_elem:
                            continue
                        link = link_elem.get('href', '')
                        
                        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                        title_elem = g.find('h3')
                        title = title_elem.get_text(strip=True) if title_elem else ''
                        
                        # –û–ø–∏—Å–∞–Ω–∏–µ
                        desc_elem = g.find('div', class_='VwiC3b')
                        desc = desc_elem.get_text(strip=True) if desc_elem else ''
                        
                        if link and (title or desc):
                            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ Telegram –∏ OLX
                            if 't.me' in link or 'olx.ua' in link:
                                results.append({
                                    'title': title,
                                    'link': link,
                                    'description': desc[:200],
                                    'query': query
                                })
                                logger.info(f"‚úì –ù–∞–π–¥–µ–Ω–æ: {title[:50]}...")
                    except Exception as e:
                        continue
                        
            elif response.status == 429:
                logger.warning("‚ö†Ô∏è Google –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª (429). –ñ–¥—ë–º...")
                await asyncio.sleep(30)
            else:
                logger.warning(f"Google —Å—Ç–∞—Ç—É—Å: {response.status}")
                
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        
    return results


async def search_duckduckgo(query, session):
    """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo HTML"""
    results = []
    
    encoded_query = urllib.parse.quote(query)
    url = f"https://html.duckduckgo.com/html/?q={encoded_query}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        async with session.get(url, headers=headers, timeout=15) as response:
            if response.status == 200:
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                for result in soup.find_all('div', class_='result'):
                    try:
                        link_elem = result.find('a', class_='result__a')
                        if not link_elem:
                            continue
                            
                        link = link_elem.get('href', '')
                        title = link_elem.get_text(strip=True)
                        
                        snippet_elem = result.find('a', class_='result__snippet')
                        snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''
                        
                        if link and ('t.me' in link or 'olx.ua' in link):
                            results.append({
                                'title': title,
                                'link': link,
                                'description': snippet[:200],
                                'query': query
                            })
                            logger.info(f"‚úì DDG: {title[:50]}...")
                    except:
                        continue
                        
    except Exception as e:
        logger.error(f"DuckDuckGo –æ—à–∏–±–∫–∞: {e}")
        
    return results


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–∏—Å–∫–∞ —Ä–∞–±–æ—Ç–Ω–∏–∫–æ–≤ —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∏")
    
    all_results = []
    
    async with aiohttp.ClientSession() as session:
        for query in SEARCH_QUERIES:
            logger.info(f"üîç –ü–æ–∏—Å–∫: {query}")
            
            # –ü—Ä–æ–±—É–µ–º DuckDuckGo (–º–µ–Ω—å—à–µ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫)
            results = await search_duckduckgo(query, session)
            all_results.extend(results)
            
            await asyncio.sleep(2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    seen = set()
    unique_results = []
    for r in all_results:
        if r['link'] not in seen:
            seen.add(r['link'])
            unique_results.append(r)
    
    logger.info(f"üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(unique_results)}")
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
    if not unique_results:
        message = "üîç –ü–æ–∏—Å–∫ —Ä–∞–±–æ—Ç–Ω–∏–∫–æ–≤\n\n"
        message += f"üìÖ {datetime.now().strftime('%d.%m.%Y %H:%M')}\n\n"
        message += "‚ùå –ß–µ—Ä–µ–∑ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n\n"
        message += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤—Ä—É—á–Ω—É—é:\n"
        message += "‚Ä¢ t.me/zhitomir9\n"
        message += "‚Ä¢ t.me/zhytomyr_olx"
    else:
        message = f"üë• –ù–∞–π–¥–µ–Ω–æ: {len(unique_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n"
        message += f"üìÖ {datetime.now().strftime('%d.%m.%Y %H:%M')}\n\n"
        
        for i, r in enumerate(unique_results[:10], 1):
            message += f"{i}. {r['title'][:60]}\n"
            message += f"   {r['description'][:80]}...\n"
            message += f"   üîó {r['link']}\n\n"
            
            if len(message) > 3500:
                break
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º
    bot = Bot(token=BOT_TOKEN)
    await bot.send_message(chat_id=CHAT_ID, text=message, disable_web_page_preview=True)
    logger.info("‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ")


if __name__ == "__main__":
    asyncio.run(main())
