"""
–ü–æ–∏—Å–∫ —Ä–µ–∑—é–º–µ (–ª—é–¥–µ–π –∏—â—É—â–∏—Ö —Ä–∞–±–æ—Ç—É) –Ω–∞ OLX
"""
import asyncio
import aiohttp
from bs4 import BeautifulSoup

async def search_olx_resumes():
    """–ò—â–µ–º –ª—é–¥–µ–π –∫–æ—Ç–æ—Ä—ã–µ –∏—â—É—Ç —Ä–∞–±–æ—Ç—É"""
    
    # –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è —Ä–µ–∑—é–º–µ
    queries = [
        "—à—É–∫–∞—é —Ä–æ–±–æ—Ç—É –∂–∏—Ç–æ–º–∏—Ä",
        "—à—É–∫–∞—é –ø—ñ–¥—Ä–æ–±—ñ—Ç–æ–∫ –∂–∏—Ç–æ–º–∏—Ä",
        "—Å–≤–∞—Ä—â–∏–∫ —à—É–∫–∞—é —Ä–æ–±–æ—Ç—É –∂–∏—Ç–æ–º–∏—Ä",
        "—Ä—ñ–∑–Ω–æ—Ä–æ–±–æ—á–∏–π —à—É–∫–∞—é —Ä–æ–±–æ—Ç—É –∂–∏—Ç–æ–º–∏—Ä",
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept-Language': 'uk-UA,uk;q=0.9',
    }
    
    all_results = []
    
    async with aiohttp.ClientSession() as session:
        for query in queries:
            import urllib.parse
            encoded = urllib.parse.quote(query)
            url = f"https://www.olx.ua/uk/list/q-{encoded}/"
            
            print(f"\nüîç –ü–æ–∏—Å–∫: {query}")
            print(f"URL: {url}")
            
            try:
                async with session.get(url, headers=headers, timeout=15) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        ads = soup.find_all('div', {'data-cy': 'l-card'})
                        print(f"–ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(ads)}")
                        
                        for ad in ads[:10]:
                            try:
                                # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                                title_elem = ad.find('h6')
                                if not title_elem:
                                    continue
                                
                                title = title_elem.get_text(strip=True)
                                
                                # –°—Å—ã–ª–∫–∞
                                link_elem = ad.find('a', href=True)
                                link = link_elem.get('href', '') if link_elem else ''
                                if link and not link.startswith('http'):
                                    link = 'https://www.olx.ua' + link
                                
                                # –í—ã–≤–æ–¥–∏–º –í–°–ï –æ–±—ä—è–≤–ª–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                                print(f"  {title[:100]}...")
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ —Ä–µ–∑—é–º–µ (–∏—â—É—Ç —Ä–∞–±–æ—Ç—É)
                                text_lower = title.lower()
                                resume_keywords = ["—à—É–∫–∞—é", "–∏—â—É", "–ø–æ—Ç—Ä—ñ–±–Ω–∞", "–Ω—É–∂–Ω–∞", "–≥–æ—Ç–æ–≤–∏–π", "–≥–æ—Ç–æ–≤"]
                                
                                if any(kw in text_lower for kw in resume_keywords):
                                    all_results.append({
                                        'title': title,
                                        'link': link,
                                        'query': query
                                    })
                                    print(f"    ‚úì –†–ï–ó–Æ–ú–ï!")
                                    
                            except Exception as e:
                                continue
                                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            
            await asyncio.sleep(2)
    
    print(f"\nüìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–∑—é–º–µ: {len(all_results)}")
    for i, r in enumerate(all_results, 1):
        print(f"{i}. {r['title']}")
        print(f"   {r['link']}\n")

asyncio.run(search_olx_resumes())
