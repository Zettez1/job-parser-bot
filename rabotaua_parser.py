"""
–ü–∞—Ä—Å–µ—Ä —Ä–µ–∑—é–º–µ —Å Rabota.ua –∏ Work.ua
–ò—â–µ—Ç: —Å–≤–∞—Ä—â–∏–∫–æ–≤, —Ä–∞–∑–Ω–æ—Ä–∞–±–æ—á–∏—Ö, –ª—é–¥–µ–π –∏—â—É—â–∏—Ö —Ä–∞–±–æ—Ç—É
"""
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from datetime import datetime
from telegram import Bot
import logging
import re

BOT_TOKEN = "8302303298:AAGH3Nllv4JaQRoi8Em8rO1-L_zGinN-gVM"
CHAT_ID = "-1003407248691"

KEYWORDS = ["—Å–≤–∞—Ä—â–∏–∫", "–∑–≤–∞—Ä–Ω–∏–∫", "—Ä–∞–∑–Ω–æ—Ä–∞–±–æ—á–∏–π", "—Ä—ñ–∑–Ω–æ—Ä–æ–±–æ—á–∏–π", "–ø—ñ–¥—Ä–æ–±—ñ—Ç–æ–∫"]

logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)


async def parse_rabotaua_resumes(session):
    """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—é–º–µ —Å Rabota.ua"""
    results = []
    
    urls = [
        "https://rabota.ua/candidates/zhitomir/%D1%81%D0%B2%D0%B0%D1%80%D1%89%D0%B8%D0%BA",  # —Å–≤–∞—Ä—â–∏–∫
        "https://rabota.ua/candidates/zhitomir/%D1%80%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9",  # —Ä–∞–±–æ—á–∏–π
        "https://rabota.ua/candidates/zhitomir",  # –≤—Å–µ —Ä–µ–∑—é–º–µ
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept-Language': 'uk-UA,uk;q=0.9',
    }
    
    for url in urls:
        try:
            logger.info(f"–ü—Ä–æ–≤–µ—Ä—è—é: {url}")
            async with session.get(url, headers=headers, timeout=15) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ —Ä–µ–∑—é–º–µ
                    cards = soup.find_all('div', class_=re.compile('.*card.*|.*resume.*'))
                    logger.info(f"–ù–∞–π–¥–µ–Ω–æ –∫–∞—Ä—Ç–æ—á–µ–∫: {len(cards)}")
                    
                    for card in cards[:15]:
                        try:
                            # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                            title_elem = card.find(['h2', 'h3', 'a'])
                            if not title_elem:
                                continue
                            
                            title = title_elem.get_text(strip=True)
                            link = title_elem.get('href', '') if title_elem.name == 'a' else ''
                            
                            if not link:
                                link_elem = card.find('a', href=True)
                                link = link_elem.get('href', '') if link_elem else ''
                            
                            if link and not link.startswith('http'):
                                link = 'https://rabota.ua' + link
                            
                            if title and len(title) > 10:
                                results.append({
                                    'text': title,
                                    'link': link or url,
                                    'source': 'Rabota.ua',
                                    'keyword': '—Ä–µ–∑—é–º–µ'
                                })
                                logger.info(f"‚úì {title[:60]}...")
                        except:
                            continue
                            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞: {e}")
        
        await asyncio.sleep(2)
    
    return results


async def parse_workua_resumes(session):
    """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—é–º–µ —Å Work.ua"""
    results = []
    
    urls = [
        "https://www.work.ua/resumes-zhytomyr/",
        "https://www.work.ua/resumes-zhytomyr-%D1%81%D0%B2%D0%B0%D1%80%D1%89%D0%B8%D0%BA/",
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept-Language': 'uk-UA,uk;q=0.9',
    }
    
    for url in urls:
        try:
            logger.info(f"–ü—Ä–æ–≤–µ—Ä—è—é: {url}")
            async with session.get(url, headers=headers, timeout=15) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # –ò—â–µ–º —Ä–µ–∑—é–º–µ
                    resumes = soup.find_all(['div', 'article'], class_=re.compile('.*resume.*|.*card.*'))
                    logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—é–º–µ: {len(resumes)}")
                    
                    for resume in resumes[:15]:
                        try:
                            title_elem = resume.find(['h2', 'h3', 'a'])
                            if not title_elem:
                                continue
                            
                            title = title_elem.get_text(strip=True)
                            link = title_elem.get('href', '') if title_elem.name == 'a' else ''
                            
                            if not link:
                                link_elem = resume.find('a', href=True)
                                link = link_elem.get('href', '') if link_elem else ''
                            
                            if link and not link.startswith('http'):
                                link = 'https://www.work.ua' + link
                            
                            if title and len(title) > 10:
                                results.append({
                                    'text': title,
                                    'link': link or url,
                                    'source': 'Work.ua',
                                    'keyword': '—Ä–µ–∑—é–º–µ'
                                })
                                logger.info(f"‚úì {title[:60]}...")
                        except:
                            continue
                            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞: {e}")
        
        await asyncio.sleep(2)
    
    return results


async def main():
    logger.info("üöÄ –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—é–º–µ —Å —Å–∞–π—Ç–æ–≤ –≤–∞–∫–∞–Ω—Å–∏–π")
    logger.info("üîç –ñ–∏—Ç–æ–º–∏—Ä: —Å–≤–∞—Ä—â–∏–∫, —Ä–∞–∑–Ω–æ—Ä–∞–±–æ—á–∏–π\n")
    
    all_results = []
    
    async with aiohttp.ClientSession() as session:
        # Rabota.ua
        logger.info("1Ô∏è‚É£ Rabota.ua...")
        results = await parse_rabotaua_resumes(session)
        all_results.extend(results)
        
        # Work.ua
        logger.info("\n2Ô∏è‚É£ Work.ua...")
        results = await parse_workua_resumes(session)
        all_results.extend(results)
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    seen = set()
    unique = []
    for r in all_results:
        if r['link'] not in seen:
            seen.add(r['link'])
            unique.append(r)
    
    logger.info(f"\nüìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(unique)} —Ä–µ–∑—é–º–µ")
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
    if unique:
        message = f"üë• –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—é–º–µ: {len(unique)}\n"
        message += f"üìÖ {datetime.now().strftime('%d.%m.%Y %H:%M')}\n"
        message += "üîç –ñ–∏—Ç–æ–º–∏—Ä - –°–≤–∞—Ä—â–∏–∫, –†–∞–∑–Ω–æ—Ä–∞–±–æ—á–∏–π\n\n"
        
        for i, r in enumerate(unique[:10], 1):
            message += f"{i}. {r['text'][:80]}\n"
            message += f"   üîó {r['link']}\n"
            message += f"   üì± {r['source']}\n\n"
            
            if len(message) > 3500:
                break
    else:
        message = "üîç –ü–æ–∏—Å–∫ —Ä–µ–∑—é–º–µ\n\n"
        message += f"üìÖ {datetime.now().strftime('%d.%m.%Y %H:%M')}\n\n"
        message += "‚ùå –†–µ–∑—é–º–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏\n\n"
        message += "üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ä—É—á–Ω—É—é:\n"
        message += "‚Ä¢ rabota.ua/candidates/zhitomir\n"
        message += "‚Ä¢ work.ua/resumes-zhytomyr/\n"
        message += "‚Ä¢ t.me/zhitomir9\n"
        message += "‚Ä¢ t.me/zhytomyr_olx"
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º
    bot = Bot(token=BOT_TOKEN)
    await bot.send_message(chat_id=CHAT_ID, text=message, disable_web_page_preview=True)
    logger.info("‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ!")


if __name__ == "__main__":
    asyncio.run(main())
