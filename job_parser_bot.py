"""
–†–ê–ë–û–ß–ò–ô –ü–ê–†–°–ï–† –†–ï–ó–Æ–ú–ï
–ò—â–µ—Ç —Å–≤–∞—Ä—â–∏–∫–æ–≤, —Ä–∞–∑–Ω–æ—Ä–∞–±–æ—á–∏—Ö –Ω–∞ Work.ua –∏ Rabota.ua
"""
import asyncio
import logging
import os
from datetime import datetime, time
import aiohttp
from aiohttp import web
from bs4 import BeautifulSoup
from telegram import Bot
from telegram.error import TelegramError
import re

# ============= –ù–ê–°–¢–†–û–ô–ö–ò =============
# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (–Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –Ω–∞ Render)
BOT_TOKEN = os.getenv("BOT_TOKEN", "8302303298:AAGH3Nllv4JaQRoi8Em8rO1-L_zGinN-gVM")
CHAT_ID = os.getenv("CHAT_ID", "-1003686632666")  # –ù–æ–≤–∞—è –≥—Ä—É–ø–ø–∞
MESSAGE_THREAD_ID = os.getenv("MESSAGE_THREAD_ID", None)  # ID —Ç–µ–º—ã HR(AI)
SEND_STARTUP_MSG = os.getenv("SEND_STARTUP_MSG", "false").lower() == "true"  # –û—Ç–ø—Ä–∞–≤–ª—è—Ç—å –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ
SEARCH_TIME = time(hour=11, minute=0)  # 11:00 UTC = 13:00 –ö–∏–µ–≤ (1 —á–∞—Å –¥–Ω—è)
PORT = int(os.getenv("PORT", 10000))

# Telegram –∫–∞–Ω–∞–ª—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (–ø—É–±–ª–∏—á–Ω—ã–µ —Å—Å—ã–ª–∫–∏)
TELEGRAM_CHANNELS = [
    "zhitomir9",  # –ñ–∏—Ç–æ–º–∏—Ä –ß–∞—Ç (1 707 —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤)
    "zhytomyr_olx",  # –ü—Ä–∞—Ü–µ–≤–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
]

# =====================================

logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)


class JobParser:
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ –ª—é–¥–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –∏—â—É—Ç —Ä–∞–±–æ—Ç—É"""
    
    def __init__(self):
        self.city = "–∂–∏—Ç–æ–º–∏—Ä"
        # –ö–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ª—é–¥–µ–π, –∏—â—É—â–∏—Ö —Ä–∞–±–æ—Ç—É
        self.job_search_keywords = [
            # –£–∫—Ä–∞–∏–Ω—Å–∫–∏–π
            "—à—É–∫–∞—é —Ä–æ–±–æ—Ç—É", "—à—É–∫–∞—é —Ä–æ–±–æ—Ç", "—à—É–∫–∞—é –ø—ñ–¥—Ä–æ–±—ñ—Ç–æ–∫", "—à—É–∫–∞—é –ø—ñ–¥–∑–∞—Ä–æ–±—ñ—Ç–æ–∫",
            "—à—É–∫–∞—é —Ä–∞–±–æ—Ç—É", "—à—É–∫–∞—é –ø–æ–¥—Ä–∞–±–æ—Ç–∫—É",
            "–ø–æ—Ç—Ä—ñ–±–Ω–∞ —Ä–æ–±–æ—Ç–∞", "–ø–æ—Ç—Ä—ñ–±–Ω–∞ —Ä–∞–±–æ—Ç–∞", "—Ç—Ä–µ–±–∞ —Ä–æ–±–æ—Ç–∞",
            "–≥–æ—Ç–æ–≤–∏–π –¥–æ —Ä–æ–±–æ—Ç–∏", "–≥–æ—Ç–æ–≤–∏–π –ø—Ä–∞—Ü—é–≤–∞—Ç–∏", "–≥–æ—Ç–æ–≤ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏",
            "—Ö–æ—á—É –ø—Ä–∞—Ü—é–≤–∞—Ç–∏", "–º–æ–∂—É –ø—Ä–∞—Ü—é–≤–∞—Ç–∏", "—Ä–æ–∑–≥–ª—è–Ω—É –ø—Ä–æ–ø–æ–∑–∏—Ü—ñ—ó",
            # –†—É—Å—Å–∫–∏–π
            "–∏—â—É —Ä–∞–±–æ—Ç—É", "–∏—â—É –ø–æ–¥—Ä–∞–±–æ—Ç–∫—É", "–∏—â—É –∑–∞—Ä–∞–±–æ—Ç–æ–∫", "–∏—â—É —Ä–æ–±–æ—Ç",
            "–Ω—É–∂–Ω–∞ —Ä–∞–±–æ—Ç–∞", "–Ω—É–∂–Ω–∞ –ø–æ–¥—Ä–∞–±–æ—Ç–∫–∞", "–Ω–∞–¥–æ —Ä–∞–±–æ—Ç—É",
            "–≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ", "–≥–æ—Ç–æ–≤ —Ä–∞–±–æ—Ç–∞—Ç—å", "—Ö–æ—á—É —Ä–∞–±–æ—Ç–∞—Ç—å", "–º–æ–≥—É —Ä–∞–±–æ—Ç–∞—Ç—å",
            "—Ä–∞—Å—Å–º–æ—Ç—Ä—é –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è", "—Ä–∞—Å—Å–º–æ—Ç—Ä—é –≤–∞—Ä–∏–∞–Ω—Ç—ã",
            # –ö–æ—Ä–æ—Ç–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
            "—à—É–∫–∞—é", "–∏—â—É", "—Ç—Ä–µ–±–∞", "–Ω—É–∂–Ω–∞", "–Ω–∞–¥–æ"
        ]
        
        # –ü—Ä–æ—Ñ–µ—Å—Å–∏–∏ –∏ –Ω–∞–≤—ã–∫–∏ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫)
        self.professions = [
            # –°–≤–∞—Ä—â–∏–∫–∏
            "—Å–≤–∞—Ä—â–∏–∫", "–∑–≤–∞—Ä–Ω–∏–∫", "—Å–≤–∞—Ä—é–≤–∞–ª—å–Ω–∏–∫", "—Å–≤–∞—Ä–æ—á–Ω–∏–∫",
            # –†–∞–∑–Ω–æ—Ä–∞–±–æ—á–∏–µ
            "—Ä–∞–∑–Ω–æ—Ä–∞–±–æ—á–∏–π", "—Ä—ñ–∑–Ω–æ—Ä–æ–±–æ—á–∏–π", "–ø–æ–¥—Å–æ–±–Ω–∏–∫", "–ø—ñ–¥—Å–æ–±–Ω–∏–∫",
            "—Ä–æ–±—ñ—Ç–Ω–∏–∫", "—Ä–∞–±–æ—á–∏–π", "–ø—Ä–∞—Ü—ñ–≤–Ω–∏–∫", "—Ä–∞–±–æ—Ç–Ω–∏–∫",
            # –°—Ç—Ä–æ–∏—Ç–µ–ª–∏
            "–±—É–¥—ñ–≤–µ–ª—å–Ω–∏–∫", "—Å—Ç—Ä–æ–∏—Ç–µ–ª—å", "–±—É–¥—ñ–≤–Ω–∏–∫", "–º–æ–Ω—Ç–∞–∂–Ω–∏–∫", "–º–æ–Ω—Ç–∞–∂–µ—Ä",
            # –í–æ–¥–∏—Ç–µ–ª–∏
            "–≤–æ–¥—ñ–π", "–≤–æ–¥–∏—Ç–µ–ª—å", "—à–æ—Ñ–µ—Ä", "–≤–æ–¥–∏—Ç–µ–ª—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏",
            # –ì—Ä—É–∑—á–∏–∫–∏
            "–≤–∞–Ω—Ç–∞–∂–Ω–∏–∫", "–≥—Ä—É–∑—á–∏–∫", "–ø–æ–≥—Ä—É–∑—á–∏–∫",
            # –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã
            "—Å–ª—é—Å–∞—Ä", "—Å–ª–µ—Å–∞—Ä—å", "—Ç–æ–∫–∞—Ä", "—Ç–æ–∫–∞—Ä—å",
            "–µ–ª–µ–∫—Ç—Ä–∏–∫", "—ç–ª–µ–∫—Ç—Ä–∏–∫", "–µ–ª–µ–∫—Ç—Ä–æ–º–æ–Ω—Ç–µ—Ä",
            "–∑–≤–∞—Ä—é–≤–∞–ª—å–Ω–∏–∫", "—Å–≤–∞—Ä—é–≤–∞–ª—å–Ω–∏–∫",
            # –î—Ä—É–≥–∏–µ —Ä–∞–±–æ—á–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏
            "—Å—Ç–æ–ª—è—Ä", "–º–∞–ª—è—Ä", "—à—Ç—É–∫–∞—Ç—É—Ä", "–ø–ª–∏—Ç–æ—á–Ω–∏–∫",
            "–º–µ—Ö–∞–Ω—ñ–∫", "–º–µ—Ö–∞–Ω–∏–∫", "–∞–≤—Ç–æ—Å–ª—é—Å–∞—Ä", "–∞–≤—Ç–æ—Å–ª–µ—Å–∞—Ä—å",
            "–æ–ø–µ—Ä–∞—Ç–æ—Ä", "—Ä—ñ–∑–Ω–∏–∫", "–º—è—Å–Ω–∏–∫"
        ]
        
    async def parse_workua_resumes(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—é–º–µ —Å Work.ua - –¢–û–õ–¨–ö–û —Å–≤–∞—Ä—â–∏–∫–∏ –∏ —Ä–∞–∑–Ω–æ—Ä–∞–±–æ—á–∏–µ"""
        results = []
        
        # –¢–û–õ–¨–ö–û –ø—Ä–æ—Ñ–∏–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        urls = [
            "https://www.work.ua/resumes-zhytomyr-%D1%81%D0%B2%D0%B0%D1%80%D1%89%D0%B8%D0%BA/",  # —Å–≤–∞—Ä—â–∏–∫
            "https://www.work.ua/resumes-zhytomyr-%D1%80%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9/",  # —Ä–∞–±–æ—á–∏–π
        ]
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–¢–û–õ–¨–ö–û –Ω—É–∂–Ω—ã–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏)
        target_keywords = [
            "—Å–≤–∞—Ä—â–∏–∫", "–∑–≤–∞—Ä–Ω–∏–∫", "—Å–≤–∞—Ä—é–≤–∞–ª—å–Ω–∏–∫", "–∑–≤–∞—Ä—é–≤–∞–ª—å–Ω–∏–∫", "–µ–ª–µ–∫—Ç—Ä–æ–∑–≤–∞—Ä–Ω–∏–∫", "—ç–ª–µ–∫—Ç—Ä–æ–∑–≤–∞—Ä—â–∏–∫",
            "—Ä–∞–∑–Ω–æ—Ä–∞–±–æ—á–∏–π", "—Ä—ñ–∑–Ω–æ—Ä–æ–±–æ—á–∏–π", "–ø–æ–¥—Å–æ–±–Ω–∏–∫", "–ø—ñ–¥—Å–æ–±–Ω–∏–∫", "—Ä–æ–±—ñ—Ç–Ω–∏–∫", "—Ä–∞–±–æ—á–∏–π"
        ]
        
        async with aiohttp.ClientSession() as session:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept-Language': 'uk-UA,uk;q=0.9',
            }
            
            for url in urls:
                try:
                    logger.info(f"Work.ua: {url}")
                    async with session.get(url, headers=headers, timeout=15) as response:
                        if response.status == 200:
                            html = await response.text()
                            soup = BeautifulSoup(html, 'html.parser')
                            
                            # –ò—â–µ–º —Ä–µ–∑—é–º–µ
                            resumes = soup.find_all(['div', 'article'], class_=re.compile('.*resume.*|.*card.*'))
                            logger.info(f"Work.ua: –Ω–∞–π–¥–µ–Ω–æ {len(resumes)} —Ä–µ–∑—é–º–µ")
                            
                            for resume in resumes[:20]:
                                try:
                                    title_elem = resume.find(['h2', 'h3', 'a'])
                                    if not title_elem:
                                        continue
                                    
                                    title = title_elem.get_text(strip=True)
                                    title_lower = title.lower()
                                    
                                    # –§–ò–õ–¨–¢–†: —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –Ω—É–∂–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                                    if not any(kw in title_lower for kw in target_keywords):
                                        continue
                                    
                                    link = title_elem.get('href', '') if title_elem.name == 'a' else ''
                                    
                                    if not link:
                                        link_elem = resume.find('a', href=True)
                                        link = link_elem.get('href', '') if link_elem else ''
                                    
                                    if link and not link.startswith('http'):
                                        link = 'https://www.work.ua' + link
                                    
                                    if title and len(title) > 10:
                                        results.append({
                                            'name': title,
                                            'link': link or url,
                                            'source': 'Work.ua'
                                        })
                                        logger.info(f"‚úì {title[:60]}...")
                                except:
                                    continue
                                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ Work.ua: {e}")
                
                await asyncio.sleep(2)
        
        return results
    
    async def parse_olx(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ OLX - —Ä–∞–∑–¥–µ–ª —Ä–µ–∑—é–º–µ (–ª—é–¥–∏ –∏—â—É—Ç —Ä–∞–±–æ—Ç—É)"""
        results = []
        try:
            # –†–∞–∑–¥–µ–ª "–†–µ–∑—é–º–µ" –Ω–∞ OLX
            url = "https://www.olx.ua/d/uk/robota/rezyume/zhitomir/"
            
            async with aiohttp.ClientSession() as session:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                async with session.get(url, headers=headers, timeout=15) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # –ò—â–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è (—É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 30)
                        ads = soup.find_all('div', {'data-cy': 'l-card'})[:30]
                        logger.info(f"OLX: –Ω–∞–π–¥–µ–Ω–æ {len(ads)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –≤ —Ä–∞–∑–¥–µ–ª–µ —Ä–µ–∑—é–º–µ")
                        
                        for ad in ads:
                            try:
                                title_elem = ad.find('h6')
                                link_elem = ad.find('a')
                                
                                if title_elem and link_elem:
                                    title = title_elem.get_text(strip=True)
                                    link = link_elem.get('href', '')
                                    
                                    if not link.startswith('http'):
                                        link = 'https://www.olx.ua' + link
                                    
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏
                                    text_lower = title.lower()
                                    has_job_search = any(kw in text_lower for kw in self.job_search_keywords)
                                    has_profession = any(prof in text_lower for prof in self.professions)
                                    
                                    # –í —Ä–∞–∑–¥–µ–ª–µ —Ä–µ–∑—é–º–µ –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã, –Ω–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                                    if has_job_search or has_profession or len(results) < 10:
                                        logger.info(f"‚úì –ù–∞–π–¥–µ–Ω–æ –Ω–∞ OLX: {title[:70]}...")
                                        results.append({
                                            'name': title,
                                            'link': link,
                                            'source': 'OLX –†–µ–∑—é–º–µ'
                                        })
                            except Exception as e:
                                logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ OLX –∫–∞—Ä—Ç–æ—á–∫–∏: {e}")
                                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ OLX: {e}")
            
        return results
    
    async def parse_rabotaua_lite(self):
        """–ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Robota.ua (–æ–±–ª–µ–≥—á—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        results = []
        try:
            url = "https://rabota.ua/zapros/zhitomir/%D1%81%D0%B2%D0%B0%D1%80%D1%89%D0%B8%D0%BA"
            
            async with aiohttp.ClientSession() as session:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                async with session.get(url, headers=headers, timeout=10) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # –ë–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏–∏
                        links = soup.find_all('a', href=re.compile(r'/company\d+/vacancy\d+'))
                        
                        for link in links[:5]:
                            try:
                                title = link.get_text(strip=True)
                                href = link.get('href', '')
                                
                                if not href.startswith('http'):
                                    href = 'https://rabota.ua' + href
                                
                                if title and len(title) > 10:
                                    results.append({
                                        'name': title,
                                        'link': href,
                                        'source': 'Rabota.ua'
                                    })
                            except Exception as e:
                                logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
                                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Rabota.ua: {e}")
            
        return results
    
    async def parse_workua_lite(self):
        """–ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Work.ua"""
        results = []
        try:
            url = "https://www.work.ua/jobs-zhytomyr/"
            
            async with aiohttp.ClientSession() as session:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                async with session.get(url, headers=headers, timeout=10) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        jobs = soup.find_all('div', class_='job-link')
                        
                        for job in jobs[:5]:
                            try:
                                link_elem = job.find('a')
                                if link_elem:
                                    title = link_elem.get_text(strip=True)
                                    link = 'https://www.work.ua' + link_elem.get('href', '')
                                    
                                    results.append({
                                        'name': title,
                                        'link': link,
                                        'source': 'Work.ua'
                                    })
                            except Exception as e:
                                logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
                                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Work.ua: {e}")
            
        return results
    
    async def parse_olx_search(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ OLX —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫ - –†–ê–ë–û–ß–ò–ô –ú–ï–¢–û–î"""
        results = []
        
        # –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        queries = [
            "—Å–≤–∞—Ä—â–∏–∫ –∂–∏—Ç–æ–º–∏—Ä",
            "—Ä—ñ–∑–Ω–æ—Ä–æ–±–æ—á–∏–π –∂–∏—Ç–æ–º–∏—Ä",
        ]
        
        async with aiohttp.ClientSession() as session:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept-Language': 'uk-UA,uk;q=0.9',
            }
            
            for query in queries:
                try:
                    import urllib.parse
                    encoded = urllib.parse.quote(query)
                    url = f"https://www.olx.ua/uk/list/q-{encoded}/"
                    
                    logger.info(f"OLX: –ø–æ–∏—Å–∫ '{query}'")
                    async with session.get(url, headers=headers, timeout=15) as response:
                        if response.status == 200:
                            html = await response.text()
                            soup = BeautifulSoup(html, 'html.parser')
                            
                            ads = soup.find_all('div', {'data-cy': 'l-card'})
                            logger.info(f"OLX: –Ω–∞–π–¥–µ–Ω–æ {len(ads)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
                            
                            for ad in ads[:10]:
                                try:
                                    title_elem = ad.find('h6')
                                    if not title_elem:
                                        continue
                                    
                                    title = title_elem.get_text(strip=True)
                                    
                                    # –°—Å—ã–ª–∫–∞
                                    link_elem = ad.find('a', href=True)
                                    link = link_elem.get('href', '') if link_elem else ''
                                    if link and not link.startswith('http'):
                                        link = 'https://www.olx.ua' + link
                                    
                                    # –§–∏–ª—å—Ç—Ä: —Ç–æ–ª—å–∫–æ —Å–≤–∞—Ä—â–∏–∫–∏ –∏ —Ä–∞–∑–Ω–æ—Ä–∞–±–æ—á–∏–µ
                                    title_lower = title.lower()
                                    target_words = ["—Å–≤–∞—Ä—â–∏–∫", "–∑–≤–∞—Ä–Ω–∏–∫", "–∑–≤–∞—Ä—é–≤–∞–ª—å–Ω–∏–∫", "—Ä—ñ–∑–Ω–æ—Ä–æ–±–æ—á–∏–π", "—Ä–∞–∑–Ω–æ—Ä–∞–±–æ—á–∏–π"]
                                    
                                    if any(w in title_lower for w in target_words):
                                        results.append({
                                            'name': title,
                                            'link': link,
                                            'source': 'OLX'
                                        })
                                        logger.info(f"‚úì {title[:60]}...")
                                        
                                except:
                                    continue
                                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ OLX: {e}")
                
                await asyncio.sleep(2)
        
        return results
    
    async def get_all_candidates(self):
        """–°–æ–±—Ä–∞—Ç—å –≤—Å–µ—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        all_candidates = []
        
        # –ü–∞—Ä—Å–∏–º Work.ua (–†–ê–ë–û–¢–ê–ï–¢!)
        logger.info("üîç –ü–∞—Ä—Å–∏–Ω–≥ Work.ua...")
        workua_results = await self.parse_workua_resumes()
        all_candidates.extend(workua_results)
        
        # –ü–∞—Ä—Å–∏–º OLX (–†–ê–ë–û–¢–ê–ï–¢!)
        logger.info("üîç –ü–∞—Ä—Å–∏–Ω–≥ OLX...")
        olx_results = await self.parse_olx_search()
        all_candidates.extend(olx_results)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Å—Å—ã–ª–∫–∞–º
        seen = set()
        unique_candidates = []
        for candidate in all_candidates:
            if candidate['link'] not in seen:
                seen.add(candidate['link'])
                unique_candidates.append(candidate)
        
        return unique_candidates


class TelegramJobBot:
    """Telegram –±–æ—Ç –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    
    def __init__(self, token, chat_id):
        self.token = token
        self.chat_id = chat_id
        self.parser = JobParser()
        
    async def send_daily_report(self):
        """–û—Ç–ø—Ä–∞–≤–∏—Ç—å –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π –æ—Ç—á–µ—Ç"""
        logger.info("–ù–∞—á–∏–Ω–∞—é –ø–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤...")
        
        candidates = await self.parser.get_all_candidates()
        
        if not candidates:
            message = f"üîç –ü–æ–∏—Å–∫ —Ä–∞–±–æ—Ç–Ω–∏–∫–æ–≤\n\n"
            message += f"üìÖ {datetime.now().strftime('%d.%m.%Y %H:%M')}\n\n"
            message += "‚ùå –†–µ–∑—é–º–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\n\n"
            message += "üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ä—É—á–Ω—É—é:\n"
            message += "‚Ä¢ work.ua/resumes-zhytomyr/\n"
            message += "‚Ä¢ t.me/zhitomir9\n"
            message += "‚Ä¢ t.me/zhytomyr_olx"
        else:
            message = f"üë• –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—é–º–µ: {len(candidates)}\n"
            message += f"üìÖ {datetime.now().strftime('%d.%m.%Y %H:%M')}\n"
            message += f"üîç –°–≤–∞—Ä—â–∏–∫ | –†–∞–∑–Ω–æ—Ä–∞–±–æ—á–∏–π | –ü—ñ–¥—Ä–æ–±—ñ—Ç–æ–∫\n\n"
            
            for i, candidate in enumerate(candidates, 1):
                message += f"{i}. {candidate['name'][:80]}\n"
                message += f"   üîó {candidate['link']}\n"
                message += f"   üì± {candidate['source']}\n\n"
                
                if len(message) > 3500:
                    message += f"... –∏ –µ—â—ë {len(candidates) - i} —Ä–µ–∑—é–º–µ"
                    break
            
            message += "\nüíº –ò—Å—Ç–æ—á–Ω–∏–∫–∏: Work.ua, OLX"
        
        try:
            bot = Bot(token=self.token)
            thread_id = int(MESSAGE_THREAD_ID) if MESSAGE_THREAD_ID else None
            await bot.send_message(
                chat_id=self.chat_id, 
                text=message, 
                disable_web_page_preview=True,
                message_thread_id=thread_id
            )
            logger.info(f"–°–æ–æ–±—â–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ" + (f" –≤ —Ç–µ–º—É {thread_id}" if thread_id else ""))
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
    
    async def scheduled_task(self):
        """–ó–∞–¥–∞—á–∞ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é"""
        while True:
            now = datetime.now().time()
            target = SEARCH_TIME
            
            current_seconds = now.hour * 3600 + now.minute * 60 + now.second
            target_seconds = target.hour * 3600 + target.minute * 60
            
            if current_seconds < target_seconds:
                wait_seconds = target_seconds - current_seconds
            else:
                wait_seconds = 86400 - current_seconds + target_seconds
            
            logger.info(f"–°–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ {wait_seconds // 3600} —á {(wait_seconds % 3600) // 60} –º–∏–Ω")
            
            await asyncio.sleep(wait_seconds)
            await self.send_daily_report()
    
    async def send_startup_message(self):
        """–û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∑–∞–ø—É—Å–∫–µ"""
        message = "ü§ñ AI Head Hunter deployed\n\n"
        message += "‚úÖ –ë–æ—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω –Ω–∞ Render\n"
        message += f"üìÖ –í—Ä–µ–º—è –æ—Ç–ø—Ä–∞–≤–∫–∏: 13:00 –ö–∏–µ–≤ (–∫–∞–∂–¥—ã–π –¥–µ–Ω—å)\n"
        message += f"üîç –ò—Å—Ç–æ—á–Ω–∏–∫–∏: Work.ua, OLX\n"
        message += f"üíº –ò—â—É: –°–≤–∞—Ä—â–∏–∫–∏, –†–∞–∑–Ω–æ—Ä–∞–±–æ—á–∏–µ\n\n"
        message += f"–°–ª–µ–¥—É—é—â–∏–π –æ—Ç—á—ë—Ç: —Å–µ–≥–æ–¥–Ω—è –≤ 13:00"
        
        try:
            bot = Bot(token=self.token)
            thread_id = int(MESSAGE_THREAD_ID) if MESSAGE_THREAD_ID else None
            await bot.send_message(
                chat_id=self.chat_id, 
                text=message,
                message_thread_id=thread_id
            )
            logger.info("‚úÖ –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è: {e}")
    
    async def run(self):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å –±–æ—Ç–∞"""
        logger.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω!")
        logger.info(f"–í—Ä–µ–º—è –æ—Ç–ø—Ä–∞–≤–∫–∏: {SEARCH_TIME} (13:00 –ö–∏–µ–≤)")
        logger.info(f"Chat ID: {self.chat_id}")
        logger.info(f"–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞–Ω–∞–ª–æ–≤: {', '.join(['@' + ch for ch in TELEGRAM_CHANNELS])}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
        test_mode = os.getenv("TEST_MODE", "false").lower() == "true"
        
        if test_mode:
            logger.info("üß™ –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú: –û—Ç–ø—Ä–∞–≤–ª—è—é —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –∑–∞–≤–µ—Ä—à–∞—é —Ä–∞–±–æ—Ç—É...")
            await self.send_daily_report()
            logger.info("‚úÖ –¢–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
            return
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ SEND_STARTUP_MSG=true
        if SEND_STARTUP_MSG:
            await self.send_startup_message()
        else:
            logger.info("–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ (SEND_STARTUP_MSG=false)")
        
        # –ñ–¥—ë–º —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
        logger.info("–û–∂–∏–¥–∞—é —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è (13:00 –ö–∏–µ–≤)...")
        
        await self.scheduled_task()


async def health_check(request):
    """Health check endpoint –¥–ª—è Render"""
    return web.Response(text="OK")


async def start_web_server():
    """–ó–∞–ø—É—Å–∫ –≤–µ–±-—Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è health check"""
    app = web.Application()
    app.router.add_get("/", health_check)
    app.router.add_get("/health", health_check)
    
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, "0.0.0.0", PORT)
    await site.start()
    logger.info(f"Web —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É {PORT}")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤–µ–±-—Å–µ—Ä–≤–µ—Ä –¥–ª—è health check
    await start_web_server()
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞
    bot = TelegramJobBot(BOT_TOKEN, CHAT_ID)
    await bot.run()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("–ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")